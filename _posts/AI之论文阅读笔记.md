---
title: AI之论文阅读笔记--李泽康
date: 2018-10-06 16:07:40
categories:
- 讲座分享
tags:
- 机器学习
---


[TOC]
## 一.论文概况
1. 论文名：《Playing Atari with Deep Reinforcement Learning》
2. 作者：Volodymyr Mnih等

## 二.论文简介及解决问题 
&emsp;&emsp;本文提出了一种深度学习方法，利用强化学习的方法，直接从高维的感知输入中学习控制策略。
<!-- more -->
&emsp;&emsp;模型是一个卷积神经网络，利用 Q-learning的一个变种来进行训练，输入是原始像素，输出是预测将来的奖励的 value function。

## 三.方式和细节
### 1.强化学习从深度学习的角度面临的极大挑战
&emsp;&emsp;（1）大部分成功的 深度学习算法都依赖于海量标注的数据， RL 算法，从另一个角度，必须从一个变换的奖励信号中进行学习，而且这种信号还经常是稀疏的，有噪声的，且是延迟的。

&emsp;&emsp;（2）大部分深度学习算法都假设 data samples 是相互独立的，然而， RL 经常遇到 高度相关的状态。

&emsp;&emsp;（3）在 RL 中数据分布随着算法学习到新的行为而改变，这对于深度学习假设固定的潜在分布是有问题的。
### 2.解决之道
&emsp;&emsp;（1）通过Q-Learning使用reward来构造标签；

&emsp;&emsp;（2）通过experience replay的方法来解决相关性及非静态分布问题.
### 3.背景知识
&emsp;&emsp;（1）Q-learning算法；

&emsp;&emsp;（2）神经网络来代替Q矩阵的方法.

## 四.其他人做的工作及弊端
&emsp;&emsp;1.之前也有人尝试用深度神经网络来估计环境environment，估值函数value function或者policy策略。

&emsp;&emsp;2.这些方法还没有拓展到非线性控制nonlinear control。

## 四.优点和缺点

### 优点
1. 采用End-to-End的训练方式，无需人工采集样本；
2. 算法具有通用性，一样的网络可以学习不同的游戏；
3. 通过不断的测试训练，可以实时生成无尽的样本用于有监督训练。

### 缺点
1. 由于输入的状态是短时的，所以只适用于处理只需短时记忆的问题，无法处理需要长时间经验的问题；
2. 使用CNN来训练不一定能够收敛，需要对网络的参数进行精良的设置才行。

## 五.总结
&emsp;&emsp;此文章采用全新的方法结合深度学习和强化学习，效果很好，具有很不错的通用性。
